{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Kishazi_UrbanicsAndras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1po2qFAaPxBGHNpS1cqMs8uQ1j0gUhX2W",
      "authorship_tag": "ABX9TyMLcAPZvk3HHntM/be8CVLK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/urbanicsandras/10a8c2789a90defef85c768649bf1d24/3-kishazi_urbanicsandras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY_4j-6oi79Q"
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model # Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization# Dense ~ Fully connected, Linear, \"MLP\"\n",
        "from tensorflow.keras.initializers import HeNormal, HeUniform\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwLlcIXPjGhf"
      },
      "source": [
        "# load data\n",
        "col_names = ['date','d_mean','d_max','d_min','d_prec']\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Deep Learning a gyakorlatban/3/eghajlati_adatsor_1901-2019+Budapest/BP_d.csv\",\n",
        "                   sep=';', usecols=[0,1,2,3,4] , header = 0, names = col_names,\n",
        "                   parse_dates=['date'])"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTXdXxXSks0D",
        "outputId": "068a54b6-4f9e-4204-edc2-59cfb2b9e12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Description of elements in the header\n",
        "# d_mean\tdaily mean temperature [°C]\n",
        "# d_max\tdaily maximum temperature [°C]\n",
        "# d_min\tdaily minimum temperature [°C]\n",
        "# d_prec\tdaily precipitation amount [mm]\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>d_mean</th>\n",
              "      <th>d_max</th>\n",
              "      <th>d_min</th>\n",
              "      <th>d_prec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901-01-01</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-9.2</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1901-01-02</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>-6.6</td>\n",
              "      <td>-11.3</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1901-01-03</td>\n",
              "      <td>-9.1</td>\n",
              "      <td>-6.6</td>\n",
              "      <td>-10.8</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1901-01-04</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-9.8</td>\n",
              "      <td>-12.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1901-01-05</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>-15.5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  d_mean  d_max  d_min  d_prec\n",
              "0 1901-01-01    -5.7   -0.4   -9.2     1.9\n",
              "1 1901-01-02    -9.3   -6.6  -11.3     0.0\n",
              "2 1901-01-03    -9.1   -6.6  -10.8     0.8\n",
              "3 1901-01-04   -11.0   -9.8  -12.4     0.2\n",
              "4 1901-01-05   -11.1   -9.0  -15.5     0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ten-n4Dk8gZ"
      },
      "source": [
        "# make 3 columns for year, month and day datas (from column date)\n",
        "data['month'] = pd.DatetimeIndex(data['date']).month\n",
        "data['day'] = pd.DatetimeIndex(data['date']).day\n",
        "data['year'] = pd.DatetimeIndex(data['date']).year"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMup5djTwIL7",
        "outputId": "958dae6d-7ccc-40a6-d8bf-66cc3f03e8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>d_mean</th>\n",
              "      <th>d_max</th>\n",
              "      <th>d_min</th>\n",
              "      <th>d_prec</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1901-01-01</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>-9.2</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1901-01-02</td>\n",
              "      <td>-9.3</td>\n",
              "      <td>-6.6</td>\n",
              "      <td>-11.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1901-01-03</td>\n",
              "      <td>-9.1</td>\n",
              "      <td>-6.6</td>\n",
              "      <td>-10.8</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1901-01-04</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-9.8</td>\n",
              "      <td>-12.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1901-01-05</td>\n",
              "      <td>-11.1</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>-15.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  d_mean  d_max  d_min  d_prec  month  day  year\n",
              "0 1901-01-01    -5.7   -0.4   -9.2     1.9      1    1  1901\n",
              "1 1901-01-02    -9.3   -6.6  -11.3     0.0      1    2  1901\n",
              "2 1901-01-03    -9.1   -6.6  -10.8     0.8      1    3  1901\n",
              "3 1901-01-04   -11.0   -9.8  -12.4     0.2      1    4  1901\n",
              "4 1901-01-05   -11.1   -9.0  -15.5     0.0      1    5  1901"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3-w0n3Vmt7m"
      },
      "source": [
        ""
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFfESriCmt5e"
      },
      "source": [
        "#data2 = pd.get_dummies(data.day, prefix='day')\n",
        "#data3 = pd.get_dummies(data.month, prefix='month')\n",
        "\n",
        "#data = pd.concat([data, data3, data2], axis=1)\n",
        "#data.head()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9qHpjLtmt3A"
      },
      "source": [
        "#data.iloc[:,7:]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-ui6m1mtv2"
      },
      "source": [
        ""
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5440QwE1RCv9"
      },
      "source": [
        "#### X features and y target\n",
        "X = data[['year','month', 'day']]\n",
        "#X = data.iloc[:,7:]\n",
        "y = data['d_mean']\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Uu9439rgcN"
      },
      "source": [
        "# make validation (20%), test (10%) and train (70%) sets \n",
        "valid_split = 0.2\n",
        "test_split  = 0.1\n",
        "v_start = int(X.shape[0]*(1-valid_split-test_split))\n",
        "t_start = int(X.shape[0]*(1-test_split))\n",
        "X_train, y_train = X[:v_start],        y[:v_start]\n",
        "X_valid, y_valid = X[v_start:t_start], y[v_start:t_start]\n",
        "X_test , y_test  = X[t_start:],        y[t_start:]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-4XVe1arjig"
      },
      "source": [
        "# scale the train, valid and test splits with scaler of train data \n",
        "### scaler = StandardScaler().fit(X_train)\n",
        "scaler = MinMaxScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train) \n",
        "X_valid = scaler.transform(X_valid) \n",
        "X_test  = scaler.transform(X_test) "
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9fj3gChg7hv"
      },
      "source": [
        "# build the neural network\n",
        "# activation function: tanh\n",
        "# optimazer: Adam, learning rate: 1e-4\n",
        "# loss function: mean_squared_error\n",
        "# 3 input neuron: year, month, day\n",
        "\n",
        "act='tanh'\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, kernel_regularizer=l2(l2=0.0005), input_shape=(3,)))\n",
        "model.add(Activation(act))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation(act))\n",
        "model.add(Dense(64))\n",
        "model.add(Dense(1))\n",
        "\n",
        "opt = Adam(lr=1e-4)\n",
        "loss = 'mean_squared_error'\n",
        "\n",
        "model.compile(loss=loss, optimizer=opt)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Guou5_pJJlO"
      },
      "source": [
        "# use early stopping\n",
        "es  = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=40, verbose=1, restore_best_weights=True)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvhQBe3glEmp",
        "outputId": "9bf715a1-26b1-4b83-bce4-b22379dfc872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# training\n",
        "# batch size: 128, epochs: 250 (but earlystopping stops earlier the training)\n",
        "network_history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=128, epochs=250, verbose=1,shuffle=True, callbacks=[es])"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 117.5319 - val_loss: 77.5030\n",
            "Epoch 2/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 73.1031 - val_loss: 75.1171\n",
            "Epoch 3/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 71.9840 - val_loss: 73.5422\n",
            "Epoch 4/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 70.9624 - val_loss: 72.8089\n",
            "Epoch 5/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 69.6768 - val_loss: 70.8356\n",
            "Epoch 6/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 67.5546 - val_loss: 68.4361\n",
            "Epoch 7/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 63.7063 - val_loss: 63.1085\n",
            "Epoch 8/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 55.5927 - val_loss: 52.0941\n",
            "Epoch 9/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 40.8243 - val_loss: 38.3847\n",
            "Epoch 10/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 26.9458 - val_loss: 29.3061\n",
            "Epoch 11/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 19.7023 - val_loss: 20.9652\n",
            "Epoch 12/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 15.9685 - val_loss: 17.0089\n",
            "Epoch 13/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 14.4148 - val_loss: 16.2304\n",
            "Epoch 14/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.8944 - val_loss: 15.0425\n",
            "Epoch 15/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.7195 - val_loss: 14.8964\n",
            "Epoch 16/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.6514 - val_loss: 14.6567\n",
            "Epoch 17/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.5889 - val_loss: 14.6709\n",
            "Epoch 18/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.5536 - val_loss: 14.8224\n",
            "Epoch 19/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.5283 - val_loss: 14.6798\n",
            "Epoch 20/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.5141 - val_loss: 14.6073\n",
            "Epoch 21/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4784 - val_loss: 14.5808\n",
            "Epoch 22/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4597 - val_loss: 14.6044\n",
            "Epoch 23/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4770 - val_loss: 14.6882\n",
            "Epoch 24/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4318 - val_loss: 14.6328\n",
            "Epoch 25/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4270 - val_loss: 14.4793\n",
            "Epoch 26/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3993 - val_loss: 14.9613\n",
            "Epoch 27/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.4296 - val_loss: 14.6364\n",
            "Epoch 28/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3991 - val_loss: 14.7386\n",
            "Epoch 29/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3858 - val_loss: 14.6109\n",
            "Epoch 30/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3910 - val_loss: 14.6812\n",
            "Epoch 31/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3988 - val_loss: 14.6296\n",
            "Epoch 32/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3787 - val_loss: 15.1021\n",
            "Epoch 33/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3594 - val_loss: 14.5763\n",
            "Epoch 34/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3608 - val_loss: 14.6648\n",
            "Epoch 35/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3783 - val_loss: 14.5636\n",
            "Epoch 36/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3604 - val_loss: 15.0798\n",
            "Epoch 37/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3554 - val_loss: 14.8177\n",
            "Epoch 38/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3482 - val_loss: 14.6845\n",
            "Epoch 39/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3467 - val_loss: 14.8063\n",
            "Epoch 40/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3487 - val_loss: 14.7199\n",
            "Epoch 41/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3672 - val_loss: 14.6027\n",
            "Epoch 42/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3572 - val_loss: 14.8252\n",
            "Epoch 43/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3364 - val_loss: 14.5178\n",
            "Epoch 44/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3432 - val_loss: 14.7616\n",
            "Epoch 45/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3530 - val_loss: 14.7425\n",
            "Epoch 46/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3533 - val_loss: 14.5229\n",
            "Epoch 47/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3355 - val_loss: 14.6028\n",
            "Epoch 48/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3473 - val_loss: 14.6577\n",
            "Epoch 49/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3368 - val_loss: 14.5255\n",
            "Epoch 50/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3363 - val_loss: 14.5746\n",
            "Epoch 51/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3289 - val_loss: 14.6321\n",
            "Epoch 52/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3243 - val_loss: 14.6382\n",
            "Epoch 53/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3340 - val_loss: 14.6002\n",
            "Epoch 54/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3391 - val_loss: 14.5349\n",
            "Epoch 55/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3408 - val_loss: 14.7681\n",
            "Epoch 56/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3075 - val_loss: 14.7323\n",
            "Epoch 57/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3103 - val_loss: 14.5802\n",
            "Epoch 58/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3202 - val_loss: 14.7881\n",
            "Epoch 59/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3282 - val_loss: 14.6138\n",
            "Epoch 60/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3220 - val_loss: 14.9344\n",
            "Epoch 61/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3048 - val_loss: 14.8863\n",
            "Epoch 62/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3067 - val_loss: 14.5488\n",
            "Epoch 63/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3116 - val_loss: 14.7640\n",
            "Epoch 64/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3055 - val_loss: 14.4653\n",
            "Epoch 65/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3103 - val_loss: 14.7214\n",
            "Epoch 66/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3066 - val_loss: 15.0692\n",
            "Epoch 67/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3307 - val_loss: 14.5144\n",
            "Epoch 68/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3155 - val_loss: 14.5049\n",
            "Epoch 69/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3265 - val_loss: 14.6735\n",
            "Epoch 70/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3052 - val_loss: 14.7003\n",
            "Epoch 71/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2799 - val_loss: 14.7219\n",
            "Epoch 72/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2922 - val_loss: 14.6010\n",
            "Epoch 73/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2885 - val_loss: 15.1021\n",
            "Epoch 74/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2991 - val_loss: 14.9463\n",
            "Epoch 75/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2879 - val_loss: 14.5725\n",
            "Epoch 76/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2948 - val_loss: 14.5365\n",
            "Epoch 77/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3057 - val_loss: 14.6039\n",
            "Epoch 78/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2978 - val_loss: 14.7727\n",
            "Epoch 79/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.3026 - val_loss: 14.7541\n",
            "Epoch 80/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2967 - val_loss: 14.5192\n",
            "Epoch 81/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2942 - val_loss: 14.5222\n",
            "Epoch 82/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2928 - val_loss: 14.6044\n",
            "Epoch 83/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2676 - val_loss: 14.5617\n",
            "Epoch 84/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2657 - val_loss: 14.5452\n",
            "Epoch 85/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2746 - val_loss: 14.6551\n",
            "Epoch 86/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2831 - val_loss: 14.6894\n",
            "Epoch 87/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2953 - val_loss: 14.5376\n",
            "Epoch 88/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2552 - val_loss: 14.5734\n",
            "Epoch 89/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2652 - val_loss: 14.6061\n",
            "Epoch 90/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2913 - val_loss: 14.6218\n",
            "Epoch 91/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2783 - val_loss: 14.4437\n",
            "Epoch 92/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2735 - val_loss: 14.5594\n",
            "Epoch 93/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2695 - val_loss: 14.6616\n",
            "Epoch 94/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2765 - val_loss: 14.5448\n",
            "Epoch 95/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2988 - val_loss: 14.5287\n",
            "Epoch 96/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2654 - val_loss: 14.5799\n",
            "Epoch 97/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2806 - val_loss: 14.6071\n",
            "Epoch 98/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2546 - val_loss: 14.7759\n",
            "Epoch 99/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2520 - val_loss: 14.4535\n",
            "Epoch 100/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2613 - val_loss: 14.7045\n",
            "Epoch 101/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2517 - val_loss: 14.4477\n",
            "Epoch 102/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2471 - val_loss: 14.5055\n",
            "Epoch 103/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2377 - val_loss: 14.6590\n",
            "Epoch 104/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2264 - val_loss: 14.5417\n",
            "Epoch 105/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2602 - val_loss: 14.5829\n",
            "Epoch 106/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2519 - val_loss: 14.4689\n",
            "Epoch 107/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2368 - val_loss: 14.6489\n",
            "Epoch 108/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2492 - val_loss: 14.7262\n",
            "Epoch 109/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2351 - val_loss: 14.5160\n",
            "Epoch 110/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2617 - val_loss: 14.6429\n",
            "Epoch 111/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2342 - val_loss: 14.6993\n",
            "Epoch 112/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2507 - val_loss: 14.3954\n",
            "Epoch 113/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2165 - val_loss: 14.7481\n",
            "Epoch 114/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2380 - val_loss: 14.6079\n",
            "Epoch 115/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2282 - val_loss: 14.3921\n",
            "Epoch 116/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2235 - val_loss: 14.4771\n",
            "Epoch 117/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2327 - val_loss: 14.5199\n",
            "Epoch 118/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2242 - val_loss: 14.5845\n",
            "Epoch 119/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2262 - val_loss: 14.3778\n",
            "Epoch 120/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2185 - val_loss: 14.4082\n",
            "Epoch 121/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2149 - val_loss: 14.6711\n",
            "Epoch 122/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2136 - val_loss: 14.5716\n",
            "Epoch 123/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2177 - val_loss: 14.5919\n",
            "Epoch 124/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2173 - val_loss: 14.3572\n",
            "Epoch 125/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2142 - val_loss: 14.3769\n",
            "Epoch 126/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2026 - val_loss: 14.5116\n",
            "Epoch 127/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2046 - val_loss: 14.5770\n",
            "Epoch 128/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2046 - val_loss: 14.4424\n",
            "Epoch 129/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1941 - val_loss: 14.3410\n",
            "Epoch 130/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2073 - val_loss: 14.4707\n",
            "Epoch 131/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2103 - val_loss: 14.4084\n",
            "Epoch 132/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1953 - val_loss: 14.6693\n",
            "Epoch 133/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1970 - val_loss: 14.4353\n",
            "Epoch 134/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2039 - val_loss: 14.3752\n",
            "Epoch 135/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1878 - val_loss: 14.4508\n",
            "Epoch 136/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2091 - val_loss: 14.5747\n",
            "Epoch 137/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1938 - val_loss: 14.4409\n",
            "Epoch 138/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1957 - val_loss: 14.6616\n",
            "Epoch 139/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1956 - val_loss: 14.4079\n",
            "Epoch 140/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1714 - val_loss: 14.3550\n",
            "Epoch 141/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1782 - val_loss: 14.6095\n",
            "Epoch 142/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1778 - val_loss: 14.4378\n",
            "Epoch 143/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1896 - val_loss: 14.5137\n",
            "Epoch 144/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2263 - val_loss: 14.4564\n",
            "Epoch 145/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.2118 - val_loss: 14.5370\n",
            "Epoch 146/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1871 - val_loss: 14.6373\n",
            "Epoch 147/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1773 - val_loss: 14.6893\n",
            "Epoch 148/250\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 13.1790 - val_loss: 14.6075\n",
            "Epoch 149/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1840 - val_loss: 14.5505\n",
            "Epoch 150/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1680 - val_loss: 14.2936\n",
            "Epoch 151/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1782 - val_loss: 14.7080\n",
            "Epoch 152/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1887 - val_loss: 14.6351\n",
            "Epoch 153/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1831 - val_loss: 14.3919\n",
            "Epoch 154/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1623 - val_loss: 14.5108\n",
            "Epoch 155/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1778 - val_loss: 14.3968\n",
            "Epoch 156/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1582 - val_loss: 14.3829\n",
            "Epoch 157/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1766 - val_loss: 14.4524\n",
            "Epoch 158/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1655 - val_loss: 14.3153\n",
            "Epoch 159/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1661 - val_loss: 14.4648\n",
            "Epoch 160/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1497 - val_loss: 14.3425\n",
            "Epoch 161/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1600 - val_loss: 14.4364\n",
            "Epoch 162/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1638 - val_loss: 14.3770\n",
            "Epoch 163/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1651 - val_loss: 14.4448\n",
            "Epoch 164/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1612 - val_loss: 14.5524\n",
            "Epoch 165/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1525 - val_loss: 14.4449\n",
            "Epoch 166/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1694 - val_loss: 14.4946\n",
            "Epoch 167/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1737 - val_loss: 14.4245\n",
            "Epoch 168/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1457 - val_loss: 14.3484\n",
            "Epoch 169/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1674 - val_loss: 14.4229\n",
            "Epoch 170/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1591 - val_loss: 14.3818\n",
            "Epoch 171/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1390 - val_loss: 14.4478\n",
            "Epoch 172/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1399 - val_loss: 14.2953\n",
            "Epoch 173/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1523 - val_loss: 14.4204\n",
            "Epoch 174/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1536 - val_loss: 14.5774\n",
            "Epoch 175/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1507 - val_loss: 14.4246\n",
            "Epoch 176/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1397 - val_loss: 14.3793\n",
            "Epoch 177/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1343 - val_loss: 14.5336\n",
            "Epoch 178/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1458 - val_loss: 14.3849\n",
            "Epoch 179/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1276 - val_loss: 14.2951\n",
            "Epoch 180/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1283 - val_loss: 14.3808\n",
            "Epoch 181/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1423 - val_loss: 14.5403\n",
            "Epoch 182/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1444 - val_loss: 14.3724\n",
            "Epoch 183/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1443 - val_loss: 14.3818\n",
            "Epoch 184/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1350 - val_loss: 14.2871\n",
            "Epoch 185/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1316 - val_loss: 14.2672\n",
            "Epoch 186/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1332 - val_loss: 14.3396\n",
            "Epoch 187/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1424 - val_loss: 14.2275\n",
            "Epoch 188/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1252 - val_loss: 14.3848\n",
            "Epoch 189/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1291 - val_loss: 14.4398\n",
            "Epoch 190/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1074 - val_loss: 14.2306\n",
            "Epoch 191/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1264 - val_loss: 14.4915\n",
            "Epoch 192/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1290 - val_loss: 14.3314\n",
            "Epoch 193/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1252 - val_loss: 14.2255\n",
            "Epoch 194/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1128 - val_loss: 14.2469\n",
            "Epoch 195/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1144 - val_loss: 14.3255\n",
            "Epoch 196/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1247 - val_loss: 14.3834\n",
            "Epoch 197/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1262 - val_loss: 14.2732\n",
            "Epoch 198/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0967 - val_loss: 14.3743\n",
            "Epoch 199/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1182 - val_loss: 14.2586\n",
            "Epoch 200/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1221 - val_loss: 14.3019\n",
            "Epoch 201/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1193 - val_loss: 14.1945\n",
            "Epoch 202/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1285 - val_loss: 14.2032\n",
            "Epoch 203/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1205 - val_loss: 14.2190\n",
            "Epoch 204/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1122 - val_loss: 14.2583\n",
            "Epoch 205/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0981 - val_loss: 14.2280\n",
            "Epoch 206/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1086 - val_loss: 14.2948\n",
            "Epoch 207/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1093 - val_loss: 14.3339\n",
            "Epoch 208/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1065 - val_loss: 14.3577\n",
            "Epoch 209/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1214 - val_loss: 14.2547\n",
            "Epoch 210/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1101 - val_loss: 14.1766\n",
            "Epoch 211/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1062 - val_loss: 14.1826\n",
            "Epoch 212/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1126 - val_loss: 14.2563\n",
            "Epoch 213/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1365 - val_loss: 14.4669\n",
            "Epoch 214/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0984 - val_loss: 14.3490\n",
            "Epoch 215/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1073 - val_loss: 14.2204\n",
            "Epoch 216/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0942 - val_loss: 14.1815\n",
            "Epoch 217/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1048 - val_loss: 14.2943\n",
            "Epoch 218/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1084 - val_loss: 14.2654\n",
            "Epoch 219/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1164 - val_loss: 14.2106\n",
            "Epoch 220/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0911 - val_loss: 14.2264\n",
            "Epoch 221/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1027 - val_loss: 14.2123\n",
            "Epoch 222/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1001 - val_loss: 14.1905\n",
            "Epoch 223/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0948 - val_loss: 14.1568\n",
            "Epoch 224/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1038 - val_loss: 14.2356\n",
            "Epoch 225/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1113 - val_loss: 14.3005\n",
            "Epoch 226/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0984 - val_loss: 14.1999\n",
            "Epoch 227/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0878 - val_loss: 14.2388\n",
            "Epoch 228/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0922 - val_loss: 14.2828\n",
            "Epoch 229/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0908 - val_loss: 14.3942\n",
            "Epoch 230/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0904 - val_loss: 14.3226\n",
            "Epoch 231/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1098 - val_loss: 14.4671\n",
            "Epoch 232/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0874 - val_loss: 14.3248\n",
            "Epoch 233/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0873 - val_loss: 14.3404\n",
            "Epoch 234/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0912 - val_loss: 14.2720\n",
            "Epoch 235/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0966 - val_loss: 14.3357\n",
            "Epoch 236/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1007 - val_loss: 14.2379\n",
            "Epoch 237/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0825 - val_loss: 14.2020\n",
            "Epoch 238/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1106 - val_loss: 14.2572\n",
            "Epoch 239/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0829 - val_loss: 14.2523\n",
            "Epoch 240/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0842 - val_loss: 14.5087\n",
            "Epoch 241/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.1142 - val_loss: 14.2345\n",
            "Epoch 242/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0755 - val_loss: 14.3902\n",
            "Epoch 243/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0859 - val_loss: 14.4696\n",
            "Epoch 244/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0801 - val_loss: 14.3666\n",
            "Epoch 245/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0836 - val_loss: 14.4329\n",
            "Epoch 246/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0913 - val_loss: 14.5542\n",
            "Epoch 247/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0861 - val_loss: 14.2231\n",
            "Epoch 248/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0899 - val_loss: 14.2832\n",
            "Epoch 249/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0685 - val_loss: 14.4441\n",
            "Epoch 250/250\n",
            "238/238 [==============================] - 1s 3ms/step - loss: 13.0702 - val_loss: 14.3241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMMK4QgnmDH4",
        "outputId": "b824125d-e732-43e4-b2e5-1f93c1eeffbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# print model parameters\n",
        "model.summary()"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 25,345\n",
            "Trainable params: 25,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sgvToFCt9Xg",
        "outputId": "68bec5d4-93a2-431d-d451-b109894d1604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# plot loss and validation loss values\n",
        "def plot_history(network_history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(network_history.history['loss'])\n",
        "    plt.plot(network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    #plt.figure()\n",
        "    #plt.xlabel('Epochs')\n",
        "    #plt.ylabel('Accuracy')\n",
        "    #plt.plot(network_history.history['accuracy'])\n",
        "    #plt.plot(network_history.history['val_accuracy'])\n",
        "    #plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    #plt.show()\n",
        "\n",
        "plot_history(network_history)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbkiLtXKucKy"
      },
      "source": [
        "# prediction for X_test set\n",
        "preds = model.predict(X_test)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5ho42_zAZB8",
        "outputId": "6e635955-100e-4d7a-f36c-1cdd4e619357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# predictions and expected values\n",
        "d = {'pred': np.array(preds.ravel()), 'true': np.array(y_test)}\n",
        "df = pd.DataFrame(data=d)\n",
        "df.head()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pred</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.370329</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.434855</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.502480</td>\n",
              "      <td>5.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.573398</td>\n",
              "      <td>5.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.647791</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       pred  true\n",
              "0  2.370329   6.0\n",
              "1  2.434855   6.2\n",
              "2  2.502480   5.4\n",
              "3  2.573398   5.3\n",
              "4  2.647791   4.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Wt6UYZlMGY",
        "outputId": "6df0baee-adb9-4a44-e1e3-c5f39555d6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(df['true'], df['pred'])"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.604352435837985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0LPWwx5FNQf"
      },
      "source": [
        "# prediction  function\n",
        "# input is a list of string dates in format: YYYY-mm-dd (year-month-day)\n",
        "# output is a dataframe of dates and the predictions\n",
        "\n",
        "def prediction(a):\n",
        "  year = []\n",
        "  month = []\n",
        "  day = []\n",
        "  for i in range(len(a)):\n",
        "    date = datetime.strptime(a[i], '%Y-%m-%d')\n",
        "    year.append(date.year)\n",
        "    month.append(date.month)\n",
        "    day.append(date.day)\n",
        "  \n",
        "  d = {'year':year, 'month':month, 'day':day}\n",
        "  df = pd.DataFrame(data=d)\n",
        "\n",
        "  df_scal = scaler.transform(df)\n",
        "  pred = model.predict(df_scal)\n",
        "\n",
        "  d2 = {'pred': np.array(pred.ravel())}\n",
        "  df2 = pd.DataFrame(data=d2)\n",
        "  df3 = pd.concat([df, df2], axis=1)\n",
        "\n",
        "  return df3"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFP9RJE91Psi"
      },
      "source": [
        "Predictions of days October 28., November 3. and Noveber 24."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rz9d-XG1jnh",
        "outputId": "85e133ce-204d-42cc-84c0-e980e82dcb53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "pred0=['2020-10-28','2020-11-03','2020-11-24']\n",
        "prediction(pred0)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>10.367250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>9.729785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>24</td>\n",
              "      <td>5.756809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day       pred\n",
              "0  2020     10   28  10.367250\n",
              "1  2020     11    3   9.729785\n",
              "2  2020     11   24   5.756809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUap_1762D40"
      },
      "source": [
        "Prediction of a day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d2k6UgG2WUh",
        "outputId": "1c9ae175-4e30-4598-c91c-4913647ee8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "pred1=['2020-12-25']\n",
        "prediction(pred1)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>12</td>\n",
              "      <td>25</td>\n",
              "      <td>1.101656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day      pred\n",
              "0  2020     12   25  1.101656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8_5hZ7R2KM_"
      },
      "source": [
        "Prediction of a week"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YRGLqnL2W1x",
        "outputId": "7e4f96a9-7ef6-4a1b-af61-e7f0309168bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "pred2=['2020-10-28','2020-10-29','2020-10-30','2020-10-31','2020-11-01','2020-11-02','2020-11-03']\n",
        "prediction(pred2)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>10.367250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>29</td>\n",
              "      <td>10.210549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>10.056715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>31</td>\n",
              "      <td>9.905712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>10.207463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9.966374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>9.729785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   year  month  day       pred\n",
              "0  2020     10   28  10.367250\n",
              "1  2020     10   29  10.210549\n",
              "2  2020     10   30  10.056715\n",
              "3  2020     10   31   9.905712\n",
              "4  2020     11    1  10.207463\n",
              "5  2020     11    2   9.966374\n",
              "6  2020     11    3   9.729785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6jHYN_b2Mbs"
      },
      "source": [
        "Prediction of a month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShuPYUYjKoGO",
        "outputId": "02521f7d-1089-4f56-c888-55ce8dd1d6e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred3 =['2020-10-28','2020-10-29','2020-10-30','2020-10-31','2020-11-01','2020-11-02','2020-11-03',\n",
        "        '2020-11-04','2020-11-05','2020-11-06','2020-11-07','2020-11-08','2020-11-09','2020-11-10',\n",
        "        '2020-11-11','2020-11-12','2020-11-13','2020-11-14','2020-11-15','2020-11-16','2020-11-17',\n",
        "        '2020-11-18','2020-11-19','2020-11-20','2020-11-21','2020-11-22','2020-11-23','2020-11-24',\n",
        "        '2020-11-25','2020-11-26','2020-11-27','2020-11-28']\n",
        "prediction(pred3)"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>28</td>\n",
              "      <td>10.367252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>29</td>\n",
              "      <td>10.210549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "      <td>10.056714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020</td>\n",
              "      <td>10</td>\n",
              "      <td>31</td>\n",
              "      <td>9.905712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>10.207462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>9.966375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>9.729788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>9.497722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>9.270184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>9.047171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>8.828663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>8.614638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>9</td>\n",
              "      <td>8.405062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>8.199892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>7.999081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>7.802577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>13</td>\n",
              "      <td>7.610323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>14</td>\n",
              "      <td>7.422253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>15</td>\n",
              "      <td>7.238308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>7.058416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>17</td>\n",
              "      <td>6.882511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>18</td>\n",
              "      <td>6.710522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>19</td>\n",
              "      <td>6.542377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>6.378005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>21</td>\n",
              "      <td>6.217333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>22</td>\n",
              "      <td>6.060290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>5.906806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>24</td>\n",
              "      <td>5.756809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>25</td>\n",
              "      <td>5.610227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>5.466994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>27</td>\n",
              "      <td>5.327041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2020</td>\n",
              "      <td>11</td>\n",
              "      <td>28</td>\n",
              "      <td>5.190300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    year  month  day       pred\n",
              "0   2020     10   28  10.367252\n",
              "1   2020     10   29  10.210549\n",
              "2   2020     10   30  10.056714\n",
              "3   2020     10   31   9.905712\n",
              "4   2020     11    1  10.207462\n",
              "5   2020     11    2   9.966375\n",
              "6   2020     11    3   9.729788\n",
              "7   2020     11    4   9.497722\n",
              "8   2020     11    5   9.270184\n",
              "9   2020     11    6   9.047171\n",
              "10  2020     11    7   8.828663\n",
              "11  2020     11    8   8.614638\n",
              "12  2020     11    9   8.405062\n",
              "13  2020     11   10   8.199892\n",
              "14  2020     11   11   7.999081\n",
              "15  2020     11   12   7.802577\n",
              "16  2020     11   13   7.610323\n",
              "17  2020     11   14   7.422253\n",
              "18  2020     11   15   7.238308\n",
              "19  2020     11   16   7.058416\n",
              "20  2020     11   17   6.882511\n",
              "21  2020     11   18   6.710522\n",
              "22  2020     11   19   6.542377\n",
              "23  2020     11   20   6.378005\n",
              "24  2020     11   21   6.217333\n",
              "25  2020     11   22   6.060290\n",
              "26  2020     11   23   5.906806\n",
              "27  2020     11   24   5.756809\n",
              "28  2020     11   25   5.610227\n",
              "29  2020     11   26   5.466994\n",
              "30  2020     11   27   5.327041\n",
              "31  2020     11   28   5.190300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5LBfHDnWX-N"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}